{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_category = pd.read_csv('category.csv')\n",
    "df_customers = pd.read_csv('customers.csv')\n",
    "df_geolocation = pd.read_csv('geolocation.csv')\n",
    "df_order_items = pd.read_csv('order_items.csv')\n",
    "df_order_payments = pd.read_csv('order_payments.csv')\n",
    "df_order_reviews = pd.read_csv('order_reviews.csv')\n",
    "df_products = pd.read_csv('products.csv')\n",
    "df_sellers = pd.read_csv('sellers.csv')\n",
    "\n",
    "#inserir o nome da base de teste \n",
    "df_teste = pd.read_csv('amostra_novos_pedidos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geolocation_c = df_geolocation.rename(columns={\n",
    "    'geolocation_zip_code_prefix': 'customer_zip_code',\n",
    "    'geolocation_lat': 'geolocation_lat_c',\n",
    "    'geolocation_lng': 'geolocation_lng_c',\n",
    "    'geolocation_city': 'geolocation_city_c',\n",
    "    'geolocation_state': 'geolocation_state_c'\n",
    "})\n",
    "\n",
    "df_geolocation_c = df_geolocation_c.drop_duplicates()\n",
    "df = df_teste.merge(df_geolocation_c, on='customer_zip_code', how='left')\n",
    "df = df.drop_duplicates(subset= ['order_id','order_item_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products = df_products.drop_duplicates(subset=['product_id'])\n",
    "df = df.merge(df_products, on='product_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sellers = df_sellers.drop_duplicates(subset=['seller_id'])\n",
    "df = df.merge(df_sellers, on='seller_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_geolocation_s = df_geolocation.rename(columns={\n",
    "    'geolocation_zip_code_prefix': 'seller_zip_code_prefix',\n",
    "    'geolocation_lat': 'geolocation_lat_s',\n",
    "    'geolocation_lng': 'geolocation_lng_s',\n",
    "    'geolocation_city': 'geolocation_city_s',\n",
    "    'geolocation_state': 'geolocation_state_s'\n",
    "})\n",
    "df_geolocation_s = df_geolocation_s.drop_duplicates()\n",
    "df = df.merge(df_geolocation_s, on='seller_zip_code_prefix', how='left')\n",
    "df = df.drop_duplicates(subset= ['order_id','order_item_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['product_volume'] = df['product_length_cm'] * df['product_height_cm'] * df['product_width_cm']\n",
    "\n",
    "# Função para calcular a distância usando a fórmula de Haversine\n",
    "def calcular_distancia(lat1, lon1, lat2, lon2):\n",
    "    # Converter as coordenadas de graus para radianos\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    # Diferenças das coordenadas\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    # Fórmula de Haversine\n",
    "    a = np.sin(dlat / 2)*2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon / 2)*2\n",
    "    a = (a**2)**(1/2)\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    \n",
    "    # Raio da Terra em quilômetros\n",
    "    raio_terra_km = 6371\n",
    "    distancia = raio_terra_km * c\n",
    "    return distancia\n",
    "\n",
    "# Aplicar a função ao DataFrame para criar a nova coluna com as distâncias\n",
    "df['distancia_km'] = df.apply(lambda row: calcular_distancia(row['geolocation_lat_c'], row['geolocation_lng_c'], row['geolocation_lat_s'], row['geolocation_lng_s']), axis=1)\n",
    "df = df.drop(columns=['geolocation_lat_c', 'geolocation_lng_c', 'geolocation_lat_s', 'geolocation_lng_s'])\n",
    "\n",
    "# Converter colunas de timestamp para datetime\n",
    "df['order_purchase_timestamp'] = pd.to_datetime(df['order_purchase_timestamp'], format='mixed', dayfirst=False)\n",
    "df['order_approved_at'] = pd.to_datetime(df['order_approved_at'],format='mixed',dayfirst=False)\n",
    "\n",
    "# Calcular o tempo de aprovação da compra em minutos\n",
    "df['purchase_to_approval_minutes'] = (df['order_approved_at'] - df['order_purchase_timestamp']).dt.total_seconds() / 86400\n",
    "\n",
    "# Dicionário de substituições para padronizar variações de nomes comuns\n",
    "substituicoes = {\n",
    "    \"sp\": \"sao paulo\",\n",
    "    \"sao paulo\": \"sao paulo\",\n",
    "    \"so paulo\": \"sao paulo\",\n",
    "    \"Sao paulo\": \"sao paulo\",\n",
    "    \"saopaulo\": \"sao paulo\",\n",
    "    \"são paulo\": \"sao paulo\",\n",
    "    \"saopaulo\": \"sao paulo\",\n",
    "    \"varzea paulista\": \"varzea paulista\"\n",
    "    # Adicione outras variações de cidade aqui, se necessário\n",
    "}\n",
    "\n",
    "# Função para aplicar as substituições específicas\n",
    "def padronizar_cidade(nome):\n",
    "    return substituicoes.get(nome, nome)  # Substitui se a cidade estiver no dicionário, caso contrário, mantém o original\n",
    "\n",
    "# Aplicando a normalização e padronização nas colunas de cidades\n",
    "df['geolocation_city_c'] = df['geolocation_city_c'].apply(padronizar_cidade)\n",
    "df['geolocation_city_s'] = df['geolocation_city_s'].apply(padronizar_cidade)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover duplicatas com base em 'order_id' e 'product_id'\n",
    "df = df.drop_duplicates(subset=['order_id', 'product_id'])\n",
    "\n",
    "# Ordenar o DataFrame por 'order_id', 'distancia_km', 'purchase_to_approval_minutes', 'freight_value' e 'product_volume' em ordem decrescente\n",
    "df = df.sort_values(\n",
    "    by=['order_id', 'distancia_km', 'purchase_to_approval_minutes', 'freight_value', 'product_volume'], \n",
    "    ascending=[False, False, False, False, False]\n",
    ")\n",
    "\n",
    "# Manter apenas a linha com os maiores valores por 'order_id'\n",
    "df = df.groupby('order_id').first().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'purchase_to_approval_minutes': 'purchase_to_approval_days'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Passo 1: Preparar o conjunto de treinamento\n",
    "treino = pd.read_csv('treino_out_deliverytime.csv')\n",
    "\n",
    "# Tratar valores ausentes nas variáveis numéricas\n",
    "numeric_cols = treino.select_dtypes(include=['float64', 'int64']).columns\n",
    "treino[numeric_cols] = treino[numeric_cols].fillna(treino[numeric_cols].mean())\n",
    "\n",
    "# Tratar valores ausentes nas variáveis categóricas, excluindo 'order_id'\n",
    "categorical_cols = treino.select_dtypes(include=['object']).columns.drop('order_id', errors='ignore')\n",
    "treino[categorical_cols] = treino[categorical_cols].fillna(treino[categorical_cols].mode().iloc[0])\n",
    "\n",
    "# Remover 'order_id' das features\n",
    "treino = treino.drop(columns=['order_id'], errors='ignore')\n",
    "\n",
    "# Passo 2: Codificar variáveis categóricas no conjunto de treinamento\n",
    "ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "treino[categorical_cols] = ordinal_encoder.fit_transform(treino[categorical_cols])\n",
    "\n",
    "# Passo 3: Separar dados para prever 'approval_to_carrier_days' e 'carrier_to_customer_days'\n",
    "X_intermediate = treino.drop(columns=['approval_to_carrier_days', 'carrier_to_customer_days', 'delivery_time (days)', 'payment_value', 'seller_city', 'seller_state', 'product_category_name', 'product_height_cm','purchase_to_approval_days'])\n",
    "y_approval = treino['approval_to_carrier_days']\n",
    "y_carrier = treino['carrier_to_customer_days']\n",
    "\n",
    "# Passo 4: Treinar modelos para 'approval_to_carrier_days' e 'carrier_to_customer_days'\n",
    "\n",
    "# Modelo para 'approval_to_carrier_days'\n",
    "model_approval = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42,\n",
    "    n_estimators=1500,         # Aumente o número de estimadores\n",
    "    learning_rate=0.008,       # Reduza a taxa de aprendizado\n",
    "    max_depth=9,              # Profundidade das árvores\n",
    "    min_child_weight=9,      # Peso mínimo por folha\n",
    "    subsample=0.6988466463923404,            # Subamostragem de linhas\n",
    "    colsample_bytree=0.8533088440608811,     # Subamostragem de colunas\n",
    "    reg_alpha=0.0009902391365606555,            # Regularização L1\n",
    "    reg_lambda=0.7565432460670021,             # Regularização L2\n",
    "    gamma=0.498568613419369                 # Reduzir a complexidade do modelo\n",
    ")\n",
    "model_approval.fit(X_intermediate, y_approval)\n",
    "\n",
    "# Modelo para 'carrier_to_customer_days'\n",
    "model_carrier = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    random_state=42,\n",
    "    n_estimators=1600,         # Aumente o número de estimadores\n",
    "    learning_rate=0.01,       # Reduza a taxa de aprendizado\n",
    "    max_depth=7,              # Profundidade das árvores\n",
    "    min_child_weight=10,      # Peso mínimo por folha\n",
    "    subsample=0.6812398856987514,            # Subamostragem de linhas\n",
    "    colsample_bytree=0.6432791404141186,     # Subamostragem de colunas\n",
    "    reg_alpha=0.48691225203383853,            # Regularização L1\n",
    "    reg_lambda=2.092554847706217,             # Regularização L2\n",
    "    gamma=0.1956394716734175                 # Reduzir a complexidade do modelo\n",
    ")\n",
    "model_carrier.fit(X_intermediate, y_carrier)\n",
    "\n",
    "\n",
    "# Passo 5: Preparar o conjunto de teste\n",
    "teste = df\n",
    "\n",
    "# Tratar valores ausentes nas variáveis numéricas\n",
    "teste_numeric_cols = teste.select_dtypes(include=['float64', 'int64']).columns\n",
    "teste[teste_numeric_cols] = teste[teste_numeric_cols].fillna(treino[numeric_cols].mean())  # Usando médias do treino\n",
    "\n",
    "# Tratar valores ausentes nas variáveis categóricas\n",
    "teste_categorical_cols = teste.select_dtypes(include=['object']).columns.drop('order_id', errors='ignore')\n",
    "teste[teste_categorical_cols] = teste[teste_categorical_cols].fillna(treino[categorical_cols].mode().iloc[0])  # Usando modos do treino\n",
    "\n",
    "# Remover 'order_id' das features\n",
    "teste = teste.drop(columns=['order_id'], errors='ignore')\n",
    "\n",
    "# Codificar variáveis categóricas no conjunto de teste\n",
    "teste_categorical_cols = [col for col in categorical_cols if col in teste.columns]\n",
    "teste[teste_categorical_cols] = ordinal_encoder.transform(teste[teste_categorical_cols])\n",
    "\n",
    "# Garantir a mesma ordem de colunas\n",
    "teste_intermediate = teste[X_intermediate.columns]\n",
    "\n",
    "# Passo 6: Prever 'approval_to_carrier_days' e 'carrier_to_customer_days' no conjunto de teste\n",
    "approval_pred = model_approval.predict(teste_intermediate)\n",
    "carrier_pred = model_carrier.predict(teste_intermediate)\n",
    "\n",
    "# Passo 7: Calcular o 'delivery_time (days)' como a soma das previsões e 'purchase_to_approval_days' do conjunto de teste\n",
    "teste_final = teste_intermediate.copy()\n",
    "teste_final['approval_to_carrier_days'] = approval_pred\n",
    "teste_final['carrier_to_customer_days'] = carrier_pred\n",
    "teste_final['purchase_to_approval_days'] = teste['purchase_to_approval_days']\n",
    "\n",
    "# Calcular 'delivery_time (days)' somando as colunas\n",
    "teste_final['delivery_time (days)'] = (\n",
    "    teste_final['purchase_to_approval_days'] +\n",
    "    teste_final['approval_to_carrier_days'] +\n",
    "    teste_final['carrier_to_customer_days']\n",
    ")\n",
    "\n",
    "# Passo 8: Criar um DataFrame com 'order_id' e 'delivery_time (days)'\n",
    "df_predictions = pd.DataFrame({\n",
    "    'order_id': 'order_ids_test',\n",
    "    'delivery_time (days)': teste_final['delivery_time (days)']\n",
    "})\n",
    "\n",
    "# Passo 9: Salvar as previsões em um arquivo CSV\n",
    "df_predictions.to_csv('previsoes.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
