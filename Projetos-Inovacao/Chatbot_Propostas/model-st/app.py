import base64
import streamlit as st
import openai
import time
import os
import json
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from pydantic import BaseModel
from google import genai
from google.genai import types
import hashlib
import io

# ==================== CONFIGURA√á√ïES E CONSTANTES ====================

SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
CONVERSATIONS_DIR = os.path.join(SCRIPT_DIR, "conversations")
os.makedirs(CONVERSATIONS_DIR, exist_ok=True)

# ==================== MODELOS DE DADOS ====================


class AssistantConfig(BaseModel):
    id: str
    name: str
    description: str
    model: str = "gpt-4-turbo-preview"
    temperature: float = 0.7
    supports_files: bool = False
    supports_code_interpreter: bool = False


class Message(BaseModel):
    role: str
    content: str
    timestamp: Optional[str] = None


class Conversation(BaseModel):
    id: str
    name: str
    messages: List[Message] = []
    assistant_key: str
    created_at: str
    updated_at: str


# ==================== ASSISTENTES DISPON√çVEIS ====================

AVAILABLE_ASSISTANTS: Dict[str, AssistantConfig] = {
    "ata_para_proposta": AssistantConfig(
        id="workflow",
        name="üîÑ Ata para Proposta (Workflow Completo)",
        description="Automatiza organiza√ß√£o de ata + pesquisa insights + cria√ß√£o de proposta",
        supports_files=True,
    ),
    "organizador_atas": AssistantConfig(
        id="asst_gl4svzGMPxoDMYskRHzK62Fk",
        name="üìã Organizador de Atas",
        description="Especialista em organizar e estruturar atas de reuni√£o",
        supports_files=True,
    ),
    "criador_propostas": AssistantConfig(
        id="asst_Fgsu6icqZ8EqjlnC89TKSLk7",
        name="üíº Criador de Propostas Comerciais",
        description="Especialista em criar propostas comerciais persuasivas",
        supports_code_interpreter=True,
    ),
    "pesquisador_insights": AssistantConfig(
        id="gemini-2.5-pro",
        name="üîç Pesquisador de Insights para Proposta",
        description="Intelig√™ncia de Mercado com Google Search (Gemini)",
    ),
    "pesquisador_tendencias": AssistantConfig(
        id="gemini-2.5-pro",
        name="üìà Pesquisador de Tend√™ncias para AT",
        description="An√°lise de Tend√™ncias de Mercado com Google Search (Gemini)",
    ),
}

DEFAULT_ASSISTANT = "ata_para_proposta"

# ==================== INICIALIZA√á√ÉO ====================


def initialize_client():
    """Inicializa o cliente OpenAI com tratamento de erros"""
    try:
        api_key = st.secrets.get("OPENAI_API_KEY") or os.environ.get("OPENAI_API_KEY")
        if not api_key:
            st.error("‚ùå API Key da OpenAI n√£o configurada!", icon="üö®")
            st.stop()
        return openai.OpenAI(api_key=api_key)
    except Exception as e:
        st.error(f"‚ùå Erro ao inicializar cliente OpenAI: {e}", icon="üö®")
        st.stop()


def initialize_session_state():
    """Inicializa todas as vari√°veis de sess√£o"""
    defaults = {
        "session_id": f"session_{int(time.time())}",
        "messages": [],
        "thread_id": None,
        "assistant_key": DEFAULT_ASSISTANT,
        "current_conversation_id": None,
        "conversations": load_all_conversations(),
        "uploaded_files": [],
        "stop_generation": False,
    }

    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value


# ==================== GERENCIAMENTO DE CONVERSAS ====================


def generate_conversation_id() -> str:
    """Gera ID √∫nico para conversas"""
    return hashlib.md5(f"{time.time()}".encode()).hexdigest()[:12]


def save_conversation(conversation: Conversation):
    """Salva conversa em arquivo JSON"""
    filepath = os.path.join(CONVERSATIONS_DIR, f"{conversation.id}.json")
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(conversation.model_dump(), f, ensure_ascii=False, indent=2)


def load_conversation(conversation_id: str) -> Optional[Conversation]:
    """Carrega conversa do arquivo"""
    filepath = os.path.join(CONVERSATIONS_DIR, f"{conversation_id}.json")
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            data = json.load(f)
            return Conversation(**data)
    except FileNotFoundError:
        return None


def load_all_conversations() -> List[Conversation]:
    """Carrega todas as conversas salvas"""
    conversations = []
    for filename in os.listdir(CONVERSATIONS_DIR):
        if filename.endswith(".json"):
            conv_id = filename.replace(".json", "")
            conv = load_conversation(conv_id)
            if conv:
                conversations.append(conv)
    return sorted(conversations, key=lambda x: x.updated_at, reverse=True)


def delete_conversation(conversation_id: str):
    """Deleta uma conversa"""
    filepath = os.path.join(CONVERSATIONS_DIR, f"{conversation_id}.json")
    if os.path.exists(filepath):
        os.remove(filepath)


def create_new_conversation() -> Conversation:
    """Cria uma nova conversa"""
    now = datetime.now().isoformat()
    return Conversation(
        id=generate_conversation_id(),
        name=f"Nova Conversa - {datetime.now().strftime('%d/%m %H:%M')}",
        assistant_key=st.session_state.assistant_key,
        created_at=now,
        updated_at=now,
    )


# ==================== UTILIT√ÅRIOS ====================


def format_message_for_download(messages: List[Message]) -> str:
    """Formata mensagens para download em Markdown"""
    output = "# Conversa Exportada\n\n"
    output += f"**Data:** {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\n\n"
    output += "---\n\n"

    for msg in messages:
        role = "üßë **Voc√™**" if msg.role == "user" else "ü§ñ **Assistente**"
        output += f"### {role}\n\n{msg.content}\n\n"
        if msg.timestamp:
            output += f"*{msg.timestamp}*\n\n"
        output += "---\n\n"

    return output


# ==================== HANDLERS DE STREAMING ====================


class StreamingEventHandler(openai.AssistantEventHandler):
    def __init__(self, text_placeholder):
        super().__init__()
        self.text_placeholder = text_placeholder
        self.full_response = ""
        self.start_time = time.time()

    def on_text_delta(self, delta, snapshot):
        if st.session_state.stop_generation:
            raise Exception("Generation stopped by user")

        self.full_response += delta.value
        self.text_placeholder.markdown(self.full_response + "‚ñå")

    def on_text_done(self, text):
        self.text_placeholder.markdown(self.full_response)

    def on_exception(self, exception):
        if "stopped by user" not in str(exception):
            st.error(f"‚ùå Erro: {exception}")

    def get_full_response(self):
        return self.full_response


# ==================== FUN√á√ïES DE PROCESSAMENTO ====================


def upload_file_to_openai(file) -> Optional[str]:
    """Upload de arquivo para OpenAI"""
    try:
        file_bytes = file.read()
        file_like = io.BytesIO(file_bytes)
        file_like.name = file.name

        uploaded_file = client.files.create(file=file_like, purpose="assistants")
        return uploaded_file.id
    except Exception as e:
        st.error(f"‚ùå Erro ao fazer upload: {e}")
        return None


def process_with_assistant(prompt: str, file_ids: Optional[List[str]] = None) -> str:
    # Get assistant configuration from session state
    assistant_info = AVAILABLE_ASSISTANTS[st.session_state.assistant_key]

    # Criar thread se n√£o existir
    if not st.session_state.thread_id:
        thread = client.beta.threads.create()
        st.session_state.thread_id = thread.id

    # Preparar mensagem
    message_params = {
        "thread_id": st.session_state.thread_id,
        "role": "user",
        "content": prompt,
    }

    # Anexar arquivos de acordo com o tipo de ferramenta do assistente
    if file_ids:
        if assistant_info.supports_code_interpreter:
            # Para assistentes com Code Interpreter (ex: an√°lise de dados, gr√°ficos)
            message_params["attachments"] = [
                {"file_id": fid, "tools": [{"type": "code_interpreter"}]}
                for fid in file_ids
            ]
        elif assistant_info.supports_files:
            # Para assistentes com Vector Store/File Search (RAG)
            message_params["attachments"] = [
                {"file_id": fid, "tools": [{"type": "file_search"}]} for fid in file_ids
            ]

    # Adicionar mensagem
    client.beta.threads.messages.create(**message_params)

    # Streaming da resposta
    response_placeholder = st.empty()
    handler = StreamingEventHandler(response_placeholder)

    with client.beta.threads.runs.stream(
        thread_id=st.session_state.thread_id,
        assistant_id=assistant_info.id,
        event_handler=handler,
    ) as stream:
        stream.until_done()

    response = handler.get_full_response()

    return response


def process_insights_research(
    contexto_negocio: str, instrucao_pesquisa: Optional[str] = None
) -> Optional[str]:
    """
    Executa pesquisa de insights usando Gemini com Google Search
    """
    try:
        # Inicializa o cliente Gemini
        gemini_client = genai.Client(
            api_key=st.secrets.get("GEMINI_API_KEY") or os.environ.get("GEMINI_API_KEY")
        )

        # System instruction para pesquisador de insights
        system_instruction = """
Voc√™ √© um Agente de IA especialista em pesquisa de mercado e intelig√™ncia de neg√≥cios para DADOS, ANALYTICS, INTELIGENCIA ARTIFICIAL e BUSINESS INTELLIGENCE.

REGRAS DE PESQUISA:

1.  **REGRA CR√çTICA DE VERACIDADE:** Voc√™ DEVE usar sua ferramenta de busca interna para encontrar fontes e links REAIS. √â terminantemente proibido inventar (alucinar) links, fontes ou URLs. A precis√£o do link √© sua prioridade m√°xima.

2.  **FLEXIBILIDADE DE FONTE:** D√™ prefer√™ncia a fontes de alta credibilidade (McKinsey, BCG, Bain, Accenture, Gartner, HBR). Contudo, um link REAL de uma fonte confi√°vel (ex: Forbes, TechCrunch, relat√≥rios de ind√∫stria) √© MIL VEZES melhor do que um link falso ou quebrado de uma fonte priorit√°ria.

3.  **FOCO:** Mantenha o foco em transforma√ß√£o digital, data-driven decision making, IA, BI, ROI de projetos de dados, tend√™ncias em analytics e casos de sucesso.

4.  **ATUALIDADE:** Priorize fontes dos √∫ltimos 36 meses, mas n√£o ignore insights fundamentais mais antigos se forem os √∫nicos dispon√≠veis.

5.  **RELEV√ÇNCIA:** Pare a pesquisa quando encontrar insights que sejam diretamente acion√°veis ou muito relevantes para o contexto do cliente, evitando informa√ß√µes gen√©ricas.

FORMATO DA RESPOSTA:

-   O campo "link" DEVE ser a URL real e verific√°vel encontrada na pesquisa.
-   Se um insight relevante for encontrado, mas um link direto e funcional n√£o puder ser verificado pela ferramenta de busca, voc√™ DEVE retornar link: null.
-   Nunca preencha o campo "link" com uma URL que voc√™ n√£o tenha verificado.

Conteudo: "descri√ß√£o detalhada do insight",
Fonte: "nome da fonte real (ex: McKinsey & Company, BCG)",
Link: "URL completa e real da fonte, ou null"
"""

        # Mensagem do usu√°rio
        contents = [
            types.Content(
                role="user",
                parts=[types.Part.from_text(text=contexto_negocio)],
            ),
        ]

        tools = [types.Tool(googleSearch=types.GoogleSearch())]

        generate_content_config = types.GenerateContentConfig(
            temperature=0.7,
            thinking_config=types.ThinkingConfig(thinking_budget=-1),
            tools=tools,
            system_instruction=[types.Part.from_text(text=system_instruction)],
        )

        # Gera a resposta com streaming
        full_response = ""
        for chunk in gemini_client.models.generate_content_stream(
            model="gemini-2.5-pro",
            contents=contents,
            config=generate_content_config,
        ):
            if hasattr(chunk, "text"):
                full_response += chunk.text

        return full_response

    except Exception as e:
        st.error(f"Erro durante a pesquisa de insights: {e}", icon="üö®")
        return None


def process_tendencias_research(contexto_negocio: str) -> Optional[str]:
    """
    Executa pesquisa de tend√™ncias usando Gemini com Google Search
    """
    try:
        # Inicializa o cliente Gemini
        gemini_client = genai.Client(
            api_key=st.secrets.get("GEMINI_API_KEY") or os.environ.get("GEMINI_API_KEY")
        )

        # System instruction para pesquisador de tend√™ncias
        system_instruction = """
# ROLE AND GOAL
Voc√™ √© um Consultor Estrat√©gico S√™nior da Poli J√∫nior, especialista em an√°lise de mercado, na metodologia "The Challenger Sale" e em posicionar solu√ß√µes de Dados & IA como alavancas de valor de neg√≥cio. Seu objetivo √© criar o conte√∫do para um "One-Slide Opener" de diagn√≥stico para uma reuni√£o de vendas consultiva. Este slide deve ensinar algo novo e valioso ao cliente sobre o mundo dele, gerar credibilidade instant√¢nea e provocar uma conversa estrat√©gica, conectando os desafios do mercado √†s solu√ß√µes que oferecemos.

# CONTEXT & KNOWLEDGE BASE
Para executar sua tarefa, voc√™ deve operar com base no seguinte conhecimento pr√©vio sobre a Poli J√∫nior e nossa metodologia:

1.  **Nossas Solu√ß√µes (em termos de neg√≥cio):**
    *   **Fase 1: Construindo a Funda√ß√£o para a Intelig√™ncia:** N√≥s resolvemos problemas de dados inacess√≠veis, silos de informa√ß√£o e processos manuais. Criamos uma "fonte √∫nica da verdade", automatizamos o trabalho repetitivo e garantimos dados confi√°veis para habilitar a inova√ß√£o e a resili√™ncia do neg√≥cio. [1, 1]
    *   **Fase 2: Gerando Vantagem Competitiva com Insights:** N√≥s transformamos dados em respostas. Ajudamos empresas a entender o comportamento do seu neg√≥cio, antecipar o futuro (demanda, riscos) e otimizar decis√µes, entregando esses insights de forma visual e acion√°vel para que possam agir com confian√ßa. [1, 1]
    *   **Fase 3: Atingindo a Vanguarda da Inova√ß√£o:** N√≥s implementamos IA de ponta para criar novas frentes de valor, como capacitar equipes com "copilotos" de IA, automatizar o atendimento ao cliente de forma personalizada e extrair intelig√™ncia de dados n√£o estruturados (imagens, voz, documentos). [1, 1]

2.  **Filosofia "The Challenger Sale":** Sua abordagem deve seguir o princ√≠pio "Teach, Tailor, Take Control". O objetivo do slide de tend√™ncias √© o "Teach" (Ensinar): introduzir uma perspectiva nova e disruptiva que cria uma "tens√£o construtiva", fazendo o cliente perceber o custo da ina√ß√£o. [1]

3.  **Estrutura do "One-Slide Opener":** Cada tend√™ncia que voc√™ criar DEVE seguir esta estrutura de tr√™s partes:
    *   **Insight Disruptivo:** Uma afirma√ß√£o provocadora que desafia o status quo e ensina algo novo sobre o setor ou a fun√ß√£o do cliente.
    *   **Ponto de Dados de Apoio:** Um dado quantitativo de uma fonte respeit√°vel (McKinsey, BCG, Bain, Gartner, Deloitte, etc.) que ancora o insight na realidade e gera credibilidade.
    *   **Conex√£o com Nossas Solu√ß√µes:** Uma ponte clara e expl√≠cita que conecta o desafio apresentado a uma de nossas fases de solu√ß√£o (listadas no item 1).

# STEP-BY-STEP INSTRUCTIONS

1.  **An√°lise do Input:** Primeiro, analise profundamente as vari√°veis de entrada fornecidas pelo usu√°rio: a empresa, o cargo do interlocutor e o segmento de atua√ß√£o.
2.  **Pesquisa Direcionada:** Realize uma pesquisa focada para encontrar as tend√™ncias e desafios mais recentes e relevantes para a intersec√ß√£o do `{SEGMENTO_DE_ATUA√á√ÉO}` e da fun√ß√£o do `{CARGO_DO_INTERLOCUTOR}`. Priorize relat√≥rios, artigos e an√°lises de fontes de consultoria de elite (McKinsey, BCG, Bain, Gartner, Deloitte) e publica√ß√µes de mercado respeitadas. Busque especificamente por dados quantific√°veis (porcentagens, valores financeiros, estat√≠sticas de mercado).
3.  **Brainstorm e Sele√ß√£o Estrat√©gica:** Com base na pesquisa, identifique de 5 a 7 tend√™ncias ou desafios potenciais. Em seguida, selecione as 3 mais potentes. Uma tend√™ncia "potente" atende a tr√™s crit√©rios:
    a. Conecta-se a uma dor de neg√≥cio clara e, preferencialmente, quantific√°vel (custo, risco, perda de oportunidade).
    b. √â um insight n√£o √≥bvio, que provavelmente o cliente ainda n√£o considerou daquela forma.
    c. Conecta-se DIRETAMENTE a pelo menos uma das nossas tr√™s fases de solu√ß√£o.
4.  **Constru√ß√£o das Tend√™ncias:** Para cada uma das 3 tend√™ncias selecionadas, redija o conte√∫do seguindo a estrutura de tr√™s partes definida na `KNOWLEDGE BASE`. Seja conciso, direto e use a linguagem de neg√≥cios. A "Conex√£o com nossas solu√ß√µes" deve ser expl√≠cita, mencionando a fase e o resultado de neg√≥cio que ela gera.
5.  **Elabora√ß√£o da Pergunta de Transi√ß√£o:** Ao final, crie a "Pergunta de Transi√ß√£o". Esta pergunta deve ser aberta, estrat√©gica e for√ßar uma escolha entre os desafios apresentados, convidando o cliente a iniciar o diagn√≥stico. Ela deve ser formulada para pivotar a conversa do mercado geral para a realidade espec√≠fica da empresa dele.

# INPUT VARIABLES
*   `{NOME_DA_EMPRESA}`: [Nome da empresa do cliente]
*   `{CARGO_DO_INTERLOCUTOR}`: [Cargo da pessoa com quem ser√° a reuni√£o]
*   `{DESCRI√á√ÉO_DA_EMPRESA}`:
*   `{SEGMENTO_DE_ATUA√á√ÉO}`:

# OUTPUT FORMAT
A sua resposta final deve ser formatada em Markdown, seguindo estritamente o modelo abaixo, sem adicionar nenhuma introdu√ß√£o ou coment√°rio fora da estrutura.

---

### **Slide de Abertura: ""**

*(Este slide deve ser apresentado em menos de tr√™s minutos, com o objetivo de provocar uma conversa estrat√©gica, n√£o de dar uma aula.)*

#### **1.**

*   **Insight:**
*   **Ponto de Dados de Apoio:**
*   **(Conex√£o com suas solu√ß√µes:**)

#### **2.**

*   **Insight:**
*   **Ponto de Dados de Apoio:**
*   **(Conex√£o com suas solu√ß√µes:**)

#### **3.**

*   **Insight:**
*   **Ponto de Dados de Apoio:**
*   **(Conex√£o com suas solu√ß√µes:**)

---

### **A Transi√ß√£o para o Di√°logo (A Pergunta Final)**

**""**

# CONSTRAINTS & GUIDELINES
*   Pense como um consultor, n√£o como um assistente de pesquisa. Seu valor est√° na s√≠ntese e na conex√£o estrat√©gica, n√£o na listagem de fatos.
*   Evite jarg√£o t√©cnico a todo custo. Fale a l√≠ngua do neg√≥cio (ROI, efici√™ncia, risco, vantagem competitiva).
*   Priorize a dor de neg√≥cio. Cada insight deve apontar para um problema que custa dinheiro, tempo ou oportunidade.
*   A "Conex√£o com nossas solu√ß√µes" √© a parte mais importante. Ela deve ser expl√≠cita e direta, mostrando ao cliente que voc√™ n√£o est√° apenas identificando problemas, mas que tem um caminho para resolv√™-los.
*   Seja conciso e impactante. Cada palavra conta.
"""

        # Mensagem do usu√°rio
        contents = [
            types.Content(
                role="user",
                parts=[types.Part.from_text(text=contexto_negocio)],
            ),
        ]

        tools = [types.Tool(googleSearch=types.GoogleSearch())]

        generate_content_config = types.GenerateContentConfig(
            temperature=0.7,
            thinking_config=types.ThinkingConfig(thinking_budget=-1),
            tools=tools,
            system_instruction=[types.Part.from_text(text=system_instruction)],
        )

        # Gera a resposta com streaming
        full_response = ""
        for chunk in gemini_client.models.generate_content_stream(
            model="gemini-2.5-pro",
            contents=contents,
            config=generate_content_config,
        ):
            if hasattr(chunk, "text"):
                full_response += chunk.text

        return full_response

    except Exception as e:
        st.error(f"Erro durante a pesquisa de tend√™ncias: {e}", icon="üö®")
        return None


def process_ata_to_proposal_workflow(user_prompt: str):
    """
    Processa o workflow completo: ata desorganizada -> ata organizada -> pesquisa insights -> proposta
    """
    try:
        # Etapa 1: Organizar a ata
        st.info("üîÑ Organizando a ata...", icon="üìù")

        # Criar thread para o organizador de atas
        thread_ata = client.beta.threads.create()

        # Adicionar mensagem do usu√°rio
        client.beta.threads.messages.create(
            thread_id=thread_ata.id, role="user", content=user_prompt
        )

        # Executar o organizador de atas
        run_ata = client.beta.threads.runs.create_and_poll(
            thread_id=thread_ata.id,
            assistant_id=AVAILABLE_ASSISTANTS["organizador_atas"].id,
        )

        # Obter a resposta organizada
        messages_ata = client.beta.threads.messages.list(thread_id=thread_ata.id)
        ata_organizada = messages_ata.data[0].content[0].text.value

        # Mostrar a ata organizada
        with st.chat_message(
            "assistant", avatar=os.path.join(SCRIPT_DIR, "assets", "img", "gpt.png")
        ):
            st.markdown("### üìã Ata Organizada")
            st.markdown(ata_organizada)

        # Adicionar ao hist√≥rico
        st.session_state.messages.append(
            {
                "role": "assistant",
                "content": f"### üìã Ata Organizada\n\n{ata_organizada}",
            }
        )

        # Etapa 2: Pesquisar Insights de Mercado
        st.info("üîÑ Pesquisando insights de mercado...", icon="üîç")

        # Extrair contexto da ata organizada para a pesquisa
        insights_response = process_insights_research(
            contexto_negocio=ata_organizada,
            instrucao_pesquisa="Pesquise insights relevantes de consultorias renomadas que possam fundamentar a proposta comercial.",
        )
        # Mostrar os insights
        if insights_response:
            with st.chat_message(
                "assistant", avatar=os.path.join(SCRIPT_DIR, "assets", "img", "gpt.png")
            ):
                st.markdown("### üîç Insights de Mercado")
                st.markdown(insights_response)

            # Adicionar ao hist√≥rico
            st.session_state.messages.append(
                {
                    "role": "assistant",
                    "content": f"### üîç Insights de Mercado\n\n{insights_response}",
                }
            )
        else:
            insights_response = "N√£o foi poss√≠vel obter insights de mercado no momento."

        # Etapa 3: Criar proposta
        st.info("üîÑ Construindo proposta comercial...", icon="üíº")

        # Criar thread para o criador de propostas
        thread_proposta = client.beta.threads.create()

        # Adicionar a ata organizada E os insights como input para a proposta
        prompt_proposta = f"""Com base na seguinte ata organizada e nos insights de mercado, crie uma proposta comercial:

**ATA ORGANIZADA:**
{ata_organizada}

**INSIGHTS DE MERCADO:**
{insights_response}"""

        client.beta.threads.messages.create(
            thread_id=thread_proposta.id,
            role="user",
            content=prompt_proposta,
        )

        # Executar o criador de propostas
        run_proposta = client.beta.threads.runs.create_and_poll(
            thread_id=thread_proposta.id,
            assistant_id=AVAILABLE_ASSISTANTS["criador_propostas"].id,
        )

        # Obter a proposta criada
        messages_proposta = client.beta.threads.messages.list(
            thread_id=thread_proposta.id
        )
        proposta_criada = messages_proposta.data[0].content[0].text.value

        # Mostrar a proposta
        with st.chat_message(
            "assistant", avatar=os.path.join(SCRIPT_DIR, "assets", "img", "gpt.png")
        ):
            st.markdown("### üíº Proposta Comercial")
            st.markdown(proposta_criada)

        # Adicionar ao hist√≥rico
        st.session_state.messages.append(
            {
                "role": "assistant",
                "content": f"### üíº Proposta Comercial\n\n{proposta_criada}",
            }
        )

        return True

    except Exception as e:
        st.error(f"Erro durante o processamento do workflow: {e}", icon="üö®")
        return False


try:
    # Inicializa o cliente OpenAI usando as secrets do Streamlit
    client = openai.OpenAI(api_key=st.secrets["OPENAI_API_KEY"])
except Exception as e:
    st.error(
        "Chave da API da OpenAI n√£o encontrada. Por favor, configure seus secrets no Streamlit Cloud.",
        icon="üö®",
    )
    st.stop()

# ==================== INTERFACE SIDEBAR ====================


def render_sidebar():
    """Renderiza sidebar com gerenciamento de conversas"""
    with st.sidebar:
        st.title("üí¨ Agente Comercial")

        # Bot√£o Nova Conversa
        if st.button("‚ûï Nova Conversa", use_container_width=True, type="primary"):
            new_conv = create_new_conversation()
            st.session_state.current_conversation_id = new_conv.id
            st.session_state.messages = []
            st.session_state.thread_id = None
            st.session_state.uploaded_files = []
            st.session_state.conversations.append(new_conv)
            save_conversation(new_conv)
            st.rerun()

        st.markdown("---")

        # Hist√≥rico de Conversas
        st.subheader("üìö Conversas Recentes")

        for conv in st.session_state.conversations[:10]:  # √öltimas 10
            col1, col2, col3 = st.columns([3, 1, 1])

            with col1:
                if st.button(
                    f"üí¨ {conv.name[:25]}...",
                    key=f"load_{conv.id}",
                    use_container_width=True,
                    type=(
                        "secondary"
                        if conv.id != st.session_state.current_conversation_id
                        else "primary"
                    ),
                ):
                    st.session_state.current_conversation_id = conv.id
                    st.session_state.messages = [msg.dict() for msg in conv.messages]
                    st.session_state.assistant_key = conv.assistant_key
                    st.rerun()

            with col2:
                if st.button("‚úèÔ∏è", key=f"rename_{conv.id}"):
                    st.session_state[f"renaming_{conv.id}"] = True

            with col3:
                if st.button("üóëÔ∏è", key=f"delete_{conv.id}"):
                    delete_conversation(conv.id)
                    st.session_state.conversations = load_all_conversations()
                    if conv.id == st.session_state.current_conversation_id:
                        st.session_state.messages = []
                        st.session_state.current_conversation_id = None
                    st.rerun()

            # Renomear
            if st.session_state.get(f"renaming_{conv.id}", False):
                new_name = st.text_input(
                    "Novo nome:", value=conv.name, key=f"new_name_{conv.id}"
                )
                if st.button("‚úÖ Salvar", key=f"save_name_{conv.id}"):
                    conv.name = new_name
                    save_conversation(conv)
                    st.session_state[f"renaming_{conv.id}"] = False
                    st.rerun()

        st.markdown("---")

        # Seletor de Assistente
        st.subheader("‚öôÔ∏è Configura√ß√µes")

        assistant_options = {
            key: assistant.name for key, assistant in AVAILABLE_ASSISTANTS.items()
        }

        selected_assistant_key = st.selectbox(
            label="ü§ñ Assistente:",
            options=assistant_options.keys(),
            format_func=lambda key: assistant_options[key],
            key="selected_assistant",
            index=list(assistant_options.keys()).index(st.session_state.assistant_key),
        )

        if selected_assistant_key != st.session_state.assistant_key:
            if st.session_state.messages:
                st.warning("‚ö†Ô∏è Mudar de assistente iniciar√° nova conversa")
                if st.button("‚úÖ Confirmar", use_container_width=True):
                    st.session_state.assistant_key = selected_assistant_key
                    new_conv = create_new_conversation()
                    st.session_state.current_conversation_id = new_conv.id
                    st.session_state.messages = []
                    st.session_state.thread_id = None
                    st.rerun()
            else:
                st.session_state.assistant_key = selected_assistant_key

        assistant_info = AVAILABLE_ASSISTANTS[st.session_state.assistant_key]
        st.info(assistant_info.description, icon="‚ÑπÔ∏è")

        # Upload de Arquivos
        if assistant_info.supports_files or assistant_info.supports_code_interpreter:
            st.markdown("---")
            st.subheader("üìé Upload de Arquivos")
            uploaded_file = st.file_uploader(
                "Anexar arquivo",
                type=["txt", "pdf", "docx", "png", "jpg", "jpeg", "csv", "json"],
                key="file_uploader",
            )

            if uploaded_file and st.button("‚¨ÜÔ∏è Enviar Arquivo"):
                with st.spinner("Fazendo upload..."):
                    file_id = upload_file_to_openai(uploaded_file)
                    if file_id:
                        st.session_state.uploaded_files.append(
                            {"name": uploaded_file.name, "id": file_id}
                        )
                        st.success(f"‚úÖ {uploaded_file.name} enviado!")

            # Arquivos enviados
            if st.session_state.uploaded_files:
                st.write("**Arquivos anexados:**")
                for file in st.session_state.uploaded_files:
                    col1, col2 = st.columns([4, 1])
                    with col1:
                        st.text(f"üìÑ {file['name']}")
                    with col2:
                        if st.button("‚ùå", key=f"remove_{file['id']}"):
                            st.session_state.uploaded_files.remove(file)
                            st.rerun()

        # Logos
        st.markdown("<br>" * 3, unsafe_allow_html=True)
        logo_col1, logo_col2, logo_col3 = st.columns([1, 2, 1])
        with logo_col2:
            st.markdown(
                """
                <div style="position: relative; display: flex; justify-content: center;">
                    <img src="data:image/png;base64,{}" width="100" />
                    <img src="data:image/png;base64,{}" width="60" 
                         style="position: absolute; bottom: -30px; right: -40px;" />
                </div>
                """.format(
                    base64.b64encode(
                        open(
                            os.path.join(SCRIPT_DIR, "assets", "img", "NDados.png"),
                            "rb",
                        ).read()
                    ).decode(),
                    base64.b64encode(
                        open(
                            os.path.join(
                                SCRIPT_DIR, "assets", "img", "Poli Junior.png"
                            ),
                            "rb",
                        ).read()
                    ).decode(),
                ),
                unsafe_allow_html=True,
            )


# ==================== INTERFACE PRINCIPAL ====================


def render_message_actions(message_index: int):
    """Renderiza a√ß√µes para cada mensagem"""
    col1, col2, col3, col4 = st.columns([1, 1, 1, 8])

    with col1:
        if st.button("üìã", key=f"copy_{message_index}", help="Copiar"):
            st.code(st.session_state.messages[message_index]["content"])


def main():
    """Fun√ß√£o principal"""
    global client

    # Inicializa√ß√µes
    client = initialize_client()
    initialize_session_state()

    # Configura√ß√£o da p√°gina
    st.set_page_config(
        page_title="Agente Comercial - NDados",
        page_icon=os.path.join(SCRIPT_DIR, "assets", "img", "NDados.png"),
        layout="wide",
        initial_sidebar_state="expanded",
    )

    # CSS (mantido do c√≥digo original)
    st.markdown(
        """
<style>
/* Importa fontes personalizadas */
@import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap');

/* Vari√°veis CSS personalizadas */
:root {
    --primary-purple: #8c52ff;
    --purple-light: #8c52ff;
    --purple-dark: #8c52ff;
    --purple-ultra-light: #8c52ff;
    --purple-semi-light: #8c52ff;
    --background-gradient: linear-gradient(135deg, #f8f7ff 0%, #f0ebff 100%);
}

/* Fundo principal com padr√£o de dados */
.main .block-container {
    background: var(--background-gradient);
    position: relative;
    font-family: 'Inter', sans-serif;
    color: #333333;
}

.main .block-container::before {
    content: '';
    position: fixed;
    top: 0;
    left: 0;
    right: 0;
    bottom: 0;
    background-image: 
        /* Padr√£o de dados/pontos pequenos */
        radial-gradient(circle at 10% 10%, var(--purple-ultra-light) 1px, transparent 1px),
        radial-gradient(circle at 30% 20%, var(--purple-ultra-light) 1.5px, transparent 1.5px),
        radial-gradient(circle at 50% 15%, var(--purple-ultra-light) 1px, transparent 1px),
        radial-gradient(circle at 70% 25%, var(--purple-ultra-light) 2px, transparent 2px),
        radial-gradient(circle at 90% 10%, var(--purple-ultra-light) 1px, transparent 1px),
        radial-gradient(circle at 85% 30%, var(--purple-ultra-light) 1.5px, transparent 1.5px),
        
        /* Segunda camada de dados */
        radial-gradient(circle at 15% 40%, var(--purple-ultra-light) 1px, transparent 1px),
        radial-gradient(circle at 35% 45%, var(--purple-ultra-light) 2px, transparent 2px),
        radial-gradient(circle at 55% 35%, var(--purple-ultra-light) 1px, transparent 1px),
        radial-gradient(circle at 75% 50%, var(--purple-ultra-light) 1.5px, transparent 1.5px),
        radial-gradient(circle at 95% 45%, var(--purple-ultra-light) 1px, transparent 1px),
        
        /* Terceira camada de dados */
        radial-gradient(circle at 20% 70%, var(--purple-ultra-light) 2px, transparent 2px),
        radial-gradient(circle at 40% 65%, var(--purple-ultra-light) 1px, transparent 1px),
        radial-gradient(circle at 60% 75%, var(--purple-ultra-light) 1.5px, transparent 1.5px),
        radial-gradient(circle at 80% 70%, var(--purple-ultra-light) 1px, transparent 1px),
        
        /* Quarta camada de dados */
        radial-gradient(circle at 25% 90%, var(--purple-ultra-light) 1px, transparent 1px),
        radial-gradient(circle at 45% 95%, var(--purple-ultra-light) 1.5px, transparent 1.5px),
        radial-gradient(circle at 65% 85%, var(--purple-ultra-light) 1px, transparent 1px),
        radial-gradient(circle at 85% 95%, var(--purple-ultra-light) 2px, transparent 2px),
        
        /* Linhas sutis conectando dados */
        linear-gradient(45deg, transparent 45%, var(--purple-ultra-light) 47%, var(--purple-ultra-light) 48%, transparent 50%),
        linear-gradient(-45deg, transparent 45%, var(--purple-ultra-light) 47%, var(--purple-ultra-light) 48%, transparent 50%);
    
    background-size: 
        /* Tamanhos variados para simular dados dispersos */
        80px 80px, 120px 120px, 90px 90px, 110px 110px, 70px 70px, 100px 100px,
        95px 95px, 130px 130px, 85px 85px, 105px 105px, 75px 75px,
        140px 140px, 80px 80px, 115px 115px, 90px 90px,
        125px 125px, 95px 95px, 85px 85px, 135px 135px,
        /* Linhas de conex√£o */
        200px 200px, 180px 180px;
    
    background-position: 
        /* Posi√ß√µes aleat√≥rias para simular distribui√ß√£o de dados */
        0 0, 25px 25px, 50px 15px, 75px 35px, 100px 10px, 125px 30px,
        15px 40px, 45px 60px, 70px 45px, 95px 65px, 120px 50px,
        20px 80px, 50px 75px, 80px 85px, 110px 80px,
        30px 100px, 60px 105px, 90px 95px, 120px 110px,
        /* Linhas */
        0 0, 100px 100px;
    
    opacity: 0.4;
    z-index: -1;
    pointer-events: none;
}

/* Adiciona padr√£o extra de dados flutuantes */
.main .block-container::after {
    content: '';
    position: fixed;
    top: 0;
    left: 0;
    right: 0;
    bottom: 0;
    background-image: 
        /* Pequenos quadrados e ret√¢ngulos representando dados */
        linear-gradient(45deg, var(--purple-ultra-light) 2px, transparent 2px),
        linear-gradient(-45deg, var(--purple-ultra-light) 1px, transparent 1px),
        /* C√≠rculos maiores ocasionais */
        radial-gradient(circle at 33% 33%, var(--purple-ultra-light) 3px, transparent 3px),
        radial-gradient(circle at 66% 66%, var(--purple-ultra-light) 2.5px, transparent 2.5px);
    background-size: 150px 150px, 180px 180px, 300px 300px, 250px 250px;
    background-position: 0 0, 50px 50px, 75px 75px, 125px 125px;
    opacity: 0.2;
    z-index: -2;
    pointer-events: none;
}

/* Remove a borda padr√£o do topo do header */
header[data-testid="stHeader"] {
    border-top: none;
    background: linear-gradient(90deg, var(--primary-purple), var(--purple-light));
    height: 4px;
}

/* Estiliza√ß√£o da sidebar */
section[data-testid="stSidebar"] {
    background: linear-gradient(180deg, #ffffff 0%, #faf9ff 100%);
    border-right: 2px solid var(--purple-ultra-light);
}

section[data-testid="stSidebar"] > div {
    background: transparent;
}

/* For√ßa texto escuro em todos os elementos */
.stApp {
    color: #333333 !important;
}

/* Bot√µes personalizados */
.stButton > button {
    background: linear-gradient(135deg, var(--primary-purple), var(--purple-light));
    color: white !important;
    border: none;
    border-radius: 12px;
    font-weight: 600;
    font-family: 'Inter', sans-serif;
    transition: all 0.3s ease;
    box-shadow: 0 4px 15px rgba(140, 82, 255, 0.3);
}

.stButton > button:hover {
    background: linear-gradient(135deg, var(--purple-dark), var(--primary-purple));
    transform: translateY(-2px);
    box-shadow: 0 6px 20px rgba(140, 82, 255, 0.4);
}

/* Selectbox personalizado */
.stSelectbox > div > div {
    background-color: white;
    border: 2px solid var(--purple-ultra-light);
    border-radius: 10px;
    transition: border-color 0.3s ease;
    color: #333333 !important;
}

.stSelectbox > div > div:focus-within {
    border-color: var(--primary-purple);
    box-shadow: 0 0 0 3px var(--purple-ultra-light);
}

/* Estiliza os containers das mensagens para se parecerem com os bal√µes de chat */
[data-testid="stChatMessage"] {
    background: rgba(255, 255, 255, 0.9);
    border: 1px solid var(--purple-ultra-light);
    border-radius: 16px;
    padding: 1.5rem;
    margin-bottom: 1rem;
    backdrop-filter: blur(10px);
    box-shadow: 0 4px 20px rgba(140, 82, 255, 0.1);
    transition: all 0.3s ease;
    color: #333333 !important;
}

[data-testid="stChatMessage"]:hover {
    box-shadow: 0 6px 25px rgba(140, 82, 255, 0.15);
    transform: translateY(-1px);
}

/* Mensagens do usu√°rio */
[data-testid="stChatMessage"]:has(img[alt*="user"]) {
    background: linear-gradient(135deg, var(--primary-purple), var(--purple-light));
    color: white !important;
    margin-left: 2rem;
}

/* Mensagens do assistente */
[data-testid="stChatMessage"]:has(img[alt*="assistant"]) {
    background: rgba(255, 255, 255, 0.95);
    margin-right: 2rem;
    color: #333333 !important;
}

/* Estilo para avatares */
img[data-testid="stAvatar"] {
    width: 40px;
    height: 40px;
    border-radius: 50%;
    border: 2px solid var(--primary-purple);
    box-shadow: 0 2px 10px rgba(140, 82, 255, 0.3);
}

/* Input de chat personalizado */
.stChatInput > div {
    background: white;
    border: 2px solid var(--purple-ultra-light);
    border-radius: 15px;
    transition: all 0.3s ease;
}

.stChatInput > div:focus-within {
    border-color: var(--primary-purple);
    box-shadow: 0 0 0 3px var(--purple-ultra-light);
}

/* T√≠tulos personalizados */
h1, h2, h3 {
    color: var(--purple-dark) !important;
    font-family: 'Inter', sans-serif;
    font-weight: 700;
}

/* Texto geral */
p, div, span, label {
    color: #333333 !important;
}

/* Alertas e notifica√ß√µes */
.stAlert {
    background: rgba(255, 255, 255, 0.9);
    border-left: 4px solid var(--primary-purple);
    border-radius: 10px;
    backdrop-filter: blur(10px);
    color: #333333 !important;
}

/* Scrollbar personalizada */
::-webkit-scrollbar {
    width: 8px;
}

::-webkit-scrollbar-track {
    background: var(--purple-ultra-light);
}

::-webkit-scrollbar-thumb {
    background: var(--primary-purple);
    border-radius: 4px;
}

::-webkit-scrollbar-thumb:hover {
    background: var(--purple-dark);
}

/* Anima√ß√£o de loading */
@keyframes pulse {
    0% { opacity: 1; }
    50% { opacity: 0.6; }
    100% { opacity: 1; }
}

.loading-text {
    animation: pulse 1.5s ease-in-out infinite;
}
</style>
    """,
        unsafe_allow_html=True,
    )

    # Renderizar sidebar
    render_sidebar()

    # Interface principal
    assistant_info = AVAILABLE_ASSISTANTS[st.session_state.assistant_key]

    st.markdown(f"## {assistant_info.name}")

    # Mensagem inicial
    if not st.session_state.messages:
        st.info(
            f"üëã Como posso ajudar hoje? Estou configurado como **{assistant_info.name}**"
        )

    # Exibir mensagens
    for idx, msg in enumerate(st.session_state.messages):
        avatar_img = (
            os.path.join(SCRIPT_DIR, "assets", "img", "user.png")
            if msg["role"] == "user"
            else os.path.join(SCRIPT_DIR, "assets", "img", "gpt.png")
        )

        with st.chat_message(msg["role"], avatar=avatar_img):
            st.markdown(msg["content"])

            # Metadados
            if msg.get("timestamp"):
                st.caption(f"üïê {msg['timestamp']}")

    # Bot√µes de controle
    col1, col2, col3, _ = st.columns([2, 2, 2, 6])
    render_message_actions(idx)

    # Bot√µes de controle
    col1, col2, col3, col4 = st.columns([2, 2, 2, 6])

    with col1:
        if st.button("‚¨áÔ∏è Exportar Chat", use_container_width=True):
            markdown_content = format_message_for_download(
                [Message(**msg) for msg in st.session_state.messages]
            )
            st.download_button(
                label="üíæ Download MD",
                data=markdown_content,
                file_name=f"conversa_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md",
                mime="text/markdown",
                use_container_width=True,
            if st.button("üîÑ Regenerar", use_container_width=True):
                # Remove √∫ltima resposta e regenera
                if len(st.session_state.messages) >= 2:
                    st.session_state.messages.pop()  # Remove resposta do assistente
                    st.rerun()
                if len(st.session_state.messages) >= 2:
                    st.session_state.messages.pop()  # Remove resposta do assistente
                    last_user_msg = st.session_state.messages[-1]["content"]
                    st.rerun()

    with col3:
        if st.session_state.get("generating", False):
            if st.button("‚èπÔ∏è Parar", use_container_width=True, type="primary"):
                st.session_state.stop_generation = True

    # Input do chat
    if prompt := st.chat_input("Digite sua mensagem aqui..."):
        st.session_state.stop_generation = False

        # Adicionar mensagem do usu√°rio
        user_message = Message(
            role="user", content=prompt, timestamp=datetime.now().strftime("%H:%M:%S")
        )
        st.session_state.messages.append(user_message.model_dump())

        # Exibir mensagem do usu√°rio
        with st.chat_message(
            "user", avatar=os.path.join(SCRIPT_DIR, "assets", "img", "user.png")
        ):
            st.markdown(prompt)

        # Processar resposta
        with st.chat_message(
            "assistant", avatar=os.path.join(SCRIPT_DIR, "assets", "img", "gpt.png")
        ):
            st.session_state.generating = True

            try:
                file_ids = [f["id"] for f in st.session_state.uploaded_files]

                if st.session_state.assistant_key == "ata_para_proposta":
                    process_ata_to_proposal_workflow(prompt)
                elif st.session_state.assistant_key == "pesquisador_insights":
                    response = process_insights_research(prompt)
                    if response:
                        st.markdown(response)
                        assistant_message = Message(
                            role="assistant",
                            content=response,
                            timestamp=datetime.now().strftime("%H:%M:%S"),
                        )
                        st.session_state.messages.append(assistant_message.model_dump())
                elif st.session_state.assistant_key == "pesquisador_tendencias":
                    response = process_tendencias_research(prompt)
                    if response:
                        st.markdown(response)
                        assistant_message = Message(
                            role="assistant",
                            content=response,
                            timestamp=datetime.now().strftime("%H:%M:%S"),
                        )
                        st.session_state.messages.append(assistant_message.model_dump())
                else:
                    response = process_with_assistant(prompt, file_ids)

                    assistant_message = Message(
                        role="assistant",
                        content=response,
                        timestamp=datetime.now().strftime("%H:%M:%S"),
                    )
                    st.session_state.messages.append(assistant_message.model_dump())

                # Salvar conversa
                if st.session_state.current_conversation_id:
                    conv = load_conversation(st.session_state.current_conversation_id)
                    if conv:
                        conv.messages = [
                            Message(**msg) for msg in st.session_state.messages
                        ]
                        conv.updated_at = datetime.now().isoformat()
                        save_conversation(conv)

            except Exception as e:
                if "stopped by user" in str(e):
                    st.warning("‚èπÔ∏è Gera√ß√£o interrompida pelo usu√°rio")
                else:
                    st.error(f"‚ùå Erro: {e}")
                st.session_state.messages.pop()  # Remove mensagem com erro

            finally:
                st.session_state.generating = False


if __name__ == "__main__":
    main()
